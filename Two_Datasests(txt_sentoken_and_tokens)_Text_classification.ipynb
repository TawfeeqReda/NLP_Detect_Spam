{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Two_Datasests(txt_sentoken and tokens)_Text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TawfeeqReda/NLP_Projects_Tawfeeq/blob/master/Two_Datasests(txt_sentoken_and_tokens)_Text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOr6Uq4E-HIX",
        "outputId": "5b961760-faa1-442a-e253-51d1659de1b0"
      },
      "source": [
        "#load google drive \r\n",
        "from google.colab import drive \r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTG_kHau-UfV",
        "outputId": "3b21b5c3-ba37-4630-c72a-fbebbe85651c"
      },
      "source": [
        "%cd /content/gdrive/My Drive/NLP_application/text_classification_project_reviwes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1wD3-hiMNebVbDtm5TDQssMeVJyUFSrop/NLP_application/text_classification_project_reviwes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH7qHjSK-ZD-"
      },
      "source": [
        " !pip install plotly --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5clflJp3-dsv",
        "outputId": "510a9a3a-c06b-46c2-bd8c-00b72e15fecb"
      },
      "source": [
        "# Text Classifiation using NLP\r\n",
        "\r\n",
        "# Importing the libraries\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import pickle \r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from sklearn.datasets import load_files\r\n",
        "nltk.download('stopwords')\r\n",
        "import pandas as pd\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MNfjXPEpIK45"
      },
      "source": [
        "# Importing the dataset\r\n",
        "reviews = load_files('txt_sentoken/')\r\n",
        "X,y = reviews.data,reviews.target\r\n",
        "\r\n",
        "\r\n",
        "# Pickling the dataset\r\n",
        "with open('X.pickle','wb') as f:\r\n",
        "    pickle.dump(X,f)\r\n",
        "    \r\n",
        "with open('y.pickle','wb') as f:\r\n",
        "    pickle.dump(y,f)\r\n",
        "\r\n",
        "# Unpickling dataset\r\n",
        "#X_in = open('X.pickle','rb')\r\n",
        "#y_in = open('y.pickle','rb')\r\n",
        "#X = pickle.load(X_in)\r\n",
        "#y = pickle.load(y_in)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS73l24P6m6n"
      },
      "source": [
        "# Importing the dataset\r\n",
        "reviews = load_files('tokens/')\r\n",
        "X,y = reviews.data,reviews.target\r\n",
        "\r\n",
        "\r\n",
        "# Pickling the dataset\r\n",
        "with open('X.pickle','wb') as f:\r\n",
        "    pickle.dump(X,f)\r\n",
        "    \r\n",
        "with open('y.pickle','wb') as f:\r\n",
        "    pickle.dump(y,f)\r\n",
        "\r\n",
        "# Unpickling dataset\r\n",
        "#X_in = open('X.pickle','rb')\r\n",
        "#y_in = open('y.pickle','rb')\r\n",
        "#X = pickle.load(X_in)\r\n",
        "#y = pickle.load(y_in)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mQ7GEdMIP0C"
      },
      "source": [
        "# Creating the corpus\r\n",
        "corpus = []\r\n",
        "for i in range(0, len(X)):\r\n",
        "    review = re.sub(r'\\W', ' ', str(X[i]))\r\n",
        "    review = review.lower()\r\n",
        "    review = re.sub(r'^br$', ' ', review)\r\n",
        "    review = re.sub(r'\\s+br\\s+',' ',review)\r\n",
        "    review = re.sub(r'\\s+[a-z]\\s+', ' ',review)\r\n",
        "    review = re.sub(r'^b\\s+', '', review)\r\n",
        "    review = re.sub(r'\\s+', ' ', review)\r\n",
        "    corpus.append(review) \r\n",
        "\r\n",
        "   \r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bemwnR_NnUvv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqcFvjmGPXgV"
      },
      "source": [
        "# Creating the BOW model\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "vectorizer = CountVectorizer(max_features = 3000, min_df = 3, max_df = 0.7, stop_words = stopwords.words('english'))\r\n",
        "X = vectorizer.fit_transform(corpus).toarray()\r\n",
        "\r\n",
        "\r\n",
        "# Creating the Tf-Idf Model\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "transformer = TfidfTransformer()\r\n",
        "X = transformer.fit_transform(X).toarray()\r\n",
        "\r\n",
        "\r\n",
        "# Creating the Tf-Idf model directly\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "#max_features = 2000\r\n",
        "vectorizer = TfidfVectorizer(max_features = 20000, min_df = 4, max_df = 0.8, stop_words = stopwords.words('english'))\r\n",
        "X = vectorizer.fit_transform(corpus).toarray()\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiXSxgch1AQv"
      },
      "source": [
        "# Creating the Tf-Idf model directly\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "#max_features = 2000\r\n",
        "vectorizer = TfidfVectorizer(max_features = 600000,  max_df = 0.7, stop_words = stopwords.words('english'))\r\n",
        "X = vectorizer.fit_transform(Data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rpWJBodstlH"
      },
      "source": [
        " #Mixture of features (range (1 to 4) grams ) \r\n",
        "tf_idf = TfidfVectorizer(ngram_range=(1, 3), max_features=600000, min_df=2, stop_words = stopwords.words('english'))\r\n",
        "X = tf_idf.fit_transform(corpus).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0mj4YPWaPD6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6TxULodZbjC"
      },
      "source": [
        "# Creating the Tf-Idf Model\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "transformer = TfidfTransformer()\r\n",
        "X = transformer.fit_transform(X).toarray()\r\n",
        "\r\n",
        "\r\n",
        "# Creating the Tf-Idf model directly\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "#tf_idf = TfidfVectorizer(ngram_range=(1, 4), max_features=6000000, min_df=2)\r\n",
        "\r\n",
        "vectorizer = TfidfVectorizer(max_features = 3000, min_df = 4, max_df = 0.8, stop_words = stopwords.words('english'))\r\n",
        "X = vectorizer.fit_transform(corpus).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E86kfaztPawp"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "# usually we split the data as 80% for training and 20% for testing\r\n",
        "text_train, text_test, sent_train, sent_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nErQQ4hHISgO"
      },
      "source": [
        "# Training the classifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "classifier = LogisticRegression()\r\n",
        "classifier.fit(text_train,sent_train)\r\n",
        "\r\n",
        "# Testing model performance\r\n",
        "sent_pred = classifier.predict(text_test)\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "cm = confusion_matrix(sent_test, sent_pred)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIAI5za5VhnM",
        "outputId": "fabb5eb5-73e8-4aad-d5bb-31cc8cf352db"
      },
      "source": [
        "print(cm[0][0])\r\n",
        "print(cm[1][1])\r\n",
        "print(cm[1][0])\r\n",
        "print(cm[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117\n",
            "111\n",
            "32\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFaNRywion6q",
        "outputId": "dbf999fa-2c49-46c6-a186-967bb592bde3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "accuracy_score(sent_test, sent_pred)*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81.42857142857143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAx4IJGhIUP5"
      },
      "source": [
        "# Saving our classifier\r\n",
        "with open('classifier.pickle','wb') as f:\r\n",
        "    pickle.dump(classifier,f)\r\n",
        "    \r\n",
        "# Saving the Tf-Idf model\r\n",
        "with open('tfidfmodel.pickle','wb') as f:\r\n",
        "    pickle.dump(vectorizer,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0YbNrxsqERP"
      },
      "source": [
        "# SVM model\r\n",
        "# Aali \r\n",
        "\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "#Linear suport vector machine model with 1000 iteration \r\n",
        "svm = LinearSVC(max_iter=10000)\r\n",
        "\r\n",
        "svm.fit(text_train,sent_train)\r\n",
        "# Testing model performance\r\n",
        "sent_pred = svm.predict(text_test)\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "cm = confusion_matrix(sent_test, sent_pred)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWPuY2cQrYRQ",
        "outputId": "44ed342a-8a37-4be8-80f4-fa36cc972418"
      },
      "source": [
        "print(cm[0][0])\r\n",
        "print(cm[1][1])\r\n",
        "print(cm[1][0])\r\n",
        "print(cm[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117\n",
            "114\n",
            "29\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZG503tIrf7H",
        "outputId": "12627ba4-e9d2-4002-e398-e51c02d323dd"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "accuracy_score(sent_test, sent_pred)*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnGlh30T2dYC"
      },
      "source": [
        "#Random Forest Model\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.datasets import make_classification \r\n",
        "text_train,sent_train = make_classification(n_samples=1000, n_features=4,\r\n",
        "                            n_informative=2, n_redundant=0,\r\n",
        "                            random_state=0, shuffle=False)\r\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=20)\r\n",
        "rf.fit(text_train,sent_train)\r\n",
        "\r\n",
        "# Testing model performance\r\n",
        "#sent_pred = rf.predict(text_test)\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "cm = confusion_matrix(sent_test, sent_pred)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-JxIqhj3dLu",
        "outputId": "dc321a01-40f5-47de-b115-68a05f9ef21f"
      },
      "source": [
        "print(cm[0][0])\r\n",
        "print(cm[1][1])\r\n",
        "print(cm[1][0])\r\n",
        "print(cm[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117\n",
            "114\n",
            "29\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi1UkdWO3fGv",
        "outputId": "006bab0d-7379-4d3d-9752-64ec5a006c62"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "accuracy_score(sent_test, sent_pred)*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81.00358422939068"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    }
  ]
}