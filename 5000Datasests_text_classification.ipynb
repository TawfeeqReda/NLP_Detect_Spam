{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5000Datasests_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TawfeeqReda/NLP_Detect_Spam/blob/master/5000Datasests_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOr6Uq4E-HIX",
        "outputId": "46e1499d-7e0b-4621-f84f-1b9ce4d1eb9c"
      },
      "source": [
        "#load google drive \r\n",
        "from google.colab import drive \r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTG_kHau-UfV",
        "outputId": "9384e00e-a5ad-4c73-8234-f81ee2337068"
      },
      "source": [
        "%cd /content/gdrive/My Drive/NLP_application/text_classification_project_reviwes"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1wD3-hiMNebVbDtm5TDQssMeVJyUFSrop/NLP_application/text_classification_project_reviwes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH7qHjSK-ZD-"
      },
      "source": [
        " !pip install plotly --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5clflJp3-dsv",
        "outputId": "efbaa607-2d53-4463-e68a-deda2b6103b0"
      },
      "source": [
        "\r\n",
        "# Importing the libraries\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import pickle \r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from sklearn.datasets import load_files\r\n",
        "nltk.download('stopwords')\r\n",
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a65uNVzNW-Db"
      },
      "source": [
        "# Importing the dataset\r\n",
        "\r\n",
        "import nltk\r\n",
        "posFilePath='rt-polaritydata/rt-polarity.pos'\r\n",
        "negFilePath='rt-polaritydata/rt-polarity.neg'\r\n",
        "\r\n",
        "with open(posFilePath, encoding=\"latin-1\") as f:\r\n",
        "    posData=f.readlines()\r\n",
        "   #X,y=  posData.posData, posData.target\r\n",
        "   #pickle.dump(posData,f)\r\n",
        "    \r\n",
        "with open(negFilePath,encoding=\"latin-1\") as f:\r\n",
        "    negData=f.readlines()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vruwOBlex0Gn",
        "outputId": "ca397e92-22b6-42bb-e8b5-c682ad7451e5"
      },
      "source": [
        "len(posData)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5331"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofNBhP3GyMPg"
      },
      "source": [
        "negData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gvjiqbhvuku"
      },
      "source": [
        "#Data cleaing\r\n",
        "for i in range(len(negData)):\r\n",
        "  negData[i] = re.sub(r'\\W', ' ', negData[i])\r\n",
        "  negData[i] = re.sub(r'^br$', ' ', negData[i])\r\n",
        "  negData[i] = re.sub(r'\\s+br\\s+',' ',negData[i])\r\n",
        "  negData[i] = re.sub(r'\\s+[a-z]\\s+', ' ',negData[i])\r\n",
        "  negData[i] = re.sub(r'\\s+', ' ',negData[i])\r\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEpGoefbyEb1"
      },
      "source": [
        "#Data cleaing\r\n",
        "for i in range(len(posData)):\r\n",
        "  posData[i] = re.sub(r'\\W', ' ', posData[i])\r\n",
        "  posData[i] = re.sub(r'^br$', ' ', posData[i])\r\n",
        "  posData[i] = re.sub(r'\\s+br\\s+',' ',posData[i])\r\n",
        "  posData[i] = re.sub(r'\\s+[a-z]\\s+', ' ',posData[i])\r\n",
        "  posData[i] = re.sub(r'\\s+', ' ',posData[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMSykrbCjwxF"
      },
      "source": [
        "#Adding text with a score for positive reviews \r\n",
        "Posdf = pd.DataFrame(\r\n",
        "    {'Text':posData,\r\n",
        "     'Score':1\r\n",
        "     })"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnPTSQxPnpSq"
      },
      "source": [
        "#Adding text with a score for negative reviews\r\n",
        "Negdf = pd.DataFrame(\r\n",
        "    {'Text':negData,\r\n",
        "     'Score':0\r\n",
        "     })"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1fHpG64ZPlg"
      },
      "source": [
        "#Add both reviews to a single dataframe to be easily used\r\n",
        "Data=pd.concat([Posdf, Negdf])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEsp-58DqSiP"
      },
      "source": [
        "tf_idf = TfidfVectorizer(ngram_range=(1, 4), max_features=6000000, min_df=2, stop_words = stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJnCxJy93Wm2"
      },
      "source": [
        "tf_idf = TfidfVectorizer(max_features=9000000, min_df=8, stop_words = stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhMJJGeq4LCG"
      },
      "source": [
        "tf_idf = TfidfVectorizer(max_features=11000000, max_df = 0.6, stop_words = stopwords.words('english'))\r\n",
        "#After cleaing the data\r\n",
        "#===Multinomial Logistic Regression===\r\n",
        "#Accuracy: 0.7569392348087022\r\n",
        "#===Multinomial Naive Bayes===\r\n",
        "#Accuracy: 0.7689422355588897\r\n",
        "#===SVM===\r\n",
        "#Accuracy: 0.7603150787696924 "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeuYhHN940vj"
      },
      "source": [
        "tf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=11000000, max_df = 0.7, stop_words = stopwords.words('english')) \r\n",
        "#===Multinomial Logistic Regression===\r\n",
        "#Accuracy: 0.7156789197299325\r\n",
        "#===Multinomial Naive Bayes===\r\n",
        "#Accuracy: 0.7284321080270068\r\n",
        "#===SVM===\r\n",
        "#Accuracy: 0.7074268567141786\r\n",
        "\r\n",
        "#After cleaing the data \r\n",
        "#===Multinomial Logistic Regression===\r\n",
        "#Accuracy: 0.7475618904726181\r\n",
        "#===Multinomial Naive Bayes===\r\n",
        "#Accuracy: 0.7730682670667667\r\n",
        "#===SVM===\r\n",
        "#Accuracy: 0.767066766691673"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XI7YkXa6EhE"
      },
      "source": [
        "tf_idf = TfidfVectorizer(max_features = 600000, min_df = 5, max_df = 0.7, stop_words = stopwords.words('english')) #good \r\n",
        "#===Multinomial Logistic Regression===\r\n",
        "#Accuracy: 0.7475618904726181\r\n",
        "#===Multinomial Naive Bayes===\r\n",
        "#Accuracy: 0.764066016504126\r\n",
        "#===SVM===\r\n",
        "#Accuracy: 0.7340585146286571\r\n",
        "#======\r\n",
        "\r\n",
        "#After cleaing the data\r\n",
        "#===Multinomial Logistic Regression===\r\n",
        "#Accuracy: 0.7475618904726181\r\n",
        "#===Multinomial Naive Bayes===\r\n",
        "#Accuracy: 0.764066016504126\r\n",
        "#===SVM===\r\n",
        "#Accuracy: 0.7340585146286571"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK17Cn0J9cjM"
      },
      "source": [
        "tf_idf = TfidfVectorizer(max_features = 600000, min_df = 3, max_df = 0.7, stop_words = stopwords.words('english'))\r\n",
        "#===Multinomial Logistic Regression===\r\n",
        "#Accuracy: 0.7614403600900225\r\n",
        "#===Multinomial Naive Bayes===\r\n",
        "#Accuracy: 0.7678169542385597\r\n",
        "#===SVM===\r\n",
        "#Accuracy: 0.7468117029257314"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAAD01zp8k0S"
      },
      "source": [
        "tf_idf = TfidfVectorizer(max_features=15000000, min_df = 10, max_df = 0.7, stop_words = stopwords.words('english')) #bad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6H0D7R02u9O"
      },
      "source": [
        "logit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', \r\n",
        "                           random_state=17, verbose=1)\r\n",
        "\r\n",
        "\r\n",
        "#Linear suport vector machine model with 1000 iteration \r\n",
        "svm = LinearSVC(max_iter=10000)\r\n",
        "#multinomial Naive Bayes\r\n",
        "nb = MultinomialNB()\r\n",
        "#Random Forest Model \r\n",
        "#rf = RandomForestClassifier(n_estimators=100, max_depth=50, max_features=3000,n_jobs=-1)\r\n",
        "\r\n",
        "#-------------------------------------------------------------------------------\r\n",
        "#Fitting the data to various machine learning models\r\n",
        "# sklearn's pipeline multinomial logistic \r\n",
        "tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \r\n",
        "                                 ('logit', logit)])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# sklearn's pipeline Multinomial Naive Bayes\r\n",
        "tfidf_mnb_pipeline = Pipeline([('tf_idf', tf_idf), \r\n",
        "                                 ('mnb',nb)])\r\n",
        "\r\n",
        "#sklearn's pipeline SVM\r\n",
        "tfidf_svm_pipeline = Pipeline([('tf_idf', tf_idf), \r\n",
        "                                 ('svm',svm)])\r\n",
        "\r\n",
        "#sklearn's random Forest\r\n",
        "\r\n",
        "#tfidf_rf_pipeline = Pipeline([('tf_idf', tf_idf),   ('rf',rf)])\r\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E86kfaztPawp"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "# usually we split the data as 80% for training and 20% for testing\r\n",
        "X = Data['Text']\r\n",
        "y= Data['Score']\r\n",
        "text_train, text_test, sent_train, sent_test = train_test_split(X, y, test_size = 0.25, random_state = 17)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nErQQ4hHISgO",
        "outputId": "0ef6995a-de60-4eb3-d2f8-4be5c286ac55"
      },
      "source": [
        "#training the models\r\n",
        "tfidf_logit_pipeline.fit(text_train, sent_train)\r\n",
        "\r\n",
        "tfidf_mnb_pipeline.fit(text_train, sent_train)\r\n",
        "\r\n",
        "X=tf_idf.transform(text_train)\r\n",
        "\r\n",
        "fit = svm.fit(X, sent_train)\r\n",
        "# tfidf_rf_pipeline.fit(y_texts, y_train)\r\n",
        "\r\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    1.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVZDpnYYucD9"
      },
      "source": [
        "#vectorizing the test dataset for testing\r\n",
        "valid_logit_pred = tfidf_logit_pipeline.predict(text_test)\r\n",
        "\r\n",
        "valid_mnb_pred = tfidf_mnb_pipeline.predict(text_test)\r\n",
        "\r\n",
        "Y=tf_idf.transform(text_test)\r\n",
        "\r\n",
        "valid_svm_pred = fit.predict(Y)\r\n",
        "\r\n",
        "#valid_rf_pred = tfidf_rf_pipeline.predict(test_texts)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7kgEPKGuqgj"
      },
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score,average_precision_score,precision_score, recall_score\r\n",
        "def print_metrics(test,valid):\r\n",
        "  #print(\"F1 score: {}\".format(f1_score(test, valid, average='macro')))\r\n",
        "  #precision=precision_score(test,valid,average=\"macro\")\r\n",
        "  #recall=recall_score(test,valid,average=\"macro\")\r\n",
        "  #print('precision-score: {0:0.2f}'.format(precision))\r\n",
        "  #print('Recall-score: {0:0.2f}'.format(recall))\r\n",
        "  print(\"Accuracy: {}\".format(accuracy_score(test, valid)))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvN2lOZHuSyi",
        "outputId": "750a60a6-c369-4ef6-8a67-83f58ca9f8e7"
      },
      "source": [
        "print(\"===Multinomial Logistic Regression===\")\r\n",
        "\r\n",
        "print_metrics(sent_test, valid_logit_pred)\r\n",
        "\r\n",
        "print(\"===Multinomial Naive Bayes===\")\r\n",
        "\r\n",
        "print_metrics(sent_test, valid_mnb_pred)\r\n",
        "\r\n",
        "print(\"===SVM===\")\r\n",
        "\r\n",
        "print_metrics(sent_test, valid_svm_pred)\r\n",
        "\r\n",
        "#print(\"===Random Forest===\")\r\n",
        "\r\n",
        "#print_metrics(y_valid, valid_rf_pred)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===Multinomial Logistic Regression===\n",
            "Accuracy: 0.7569392348087022\n",
            "===Multinomial Naive Bayes===\n",
            "Accuracy: 0.7689422355588897\n",
            "===SVM===\n",
            "Accuracy: 0.7603150787696924\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}